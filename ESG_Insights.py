# -*- coding: utf-8 -*-
"""NLP_Project_Final1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qDrzOopAqawbTe6QaFWtSewUvnzB-m-v
"""

# Install compatible versions
!pip install --quiet sentence-transformers==2.2.2 transformers==4.44.0 huggingface-hub==0.14.1

!pip install huggingface-hub==0.14.1

# ==================== ESGInsight (Colab Compatible Fixed) ====================

import os, re, json, warnings
warnings.filterwarnings("ignore")

# ------------------ 1) Install Dependencies ------------------
!pip install --quiet \
    "scipy<1.12" gensim==4.3.2 nltk==3.8.1 spacy==3.7.2 \
    sentence-transformers==2.2.2 transformers==4.44.0 \
    matplotlib==3.8.4 seaborn==0.13.2 wordcloud==1.9.3 \
    yfinance==0.2.43 tqdm==4.66.4 networkx==3.3 pymupdf==1.24.9 \
    plotly==5.24.1 huggingface-hub==0.24.6

# ‚úÖ Correct way to download SpaCy model
!python -m spacy download en_core_web_sm

import nltk
for pkg in ["punkt","stopwords","wordnet"]:
    nltk.download(pkg, quiet=True)

# ------------------ 2) Imports ------------------
import fitz  # PyMuPDF
import numpy as np, pandas as pd
import spacy
from tqdm.notebook import tqdm
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from gensim import corpora, models
from sentence_transformers import SentenceTransformer
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    AutoModelForSeq2SeqLM,
    pipeline
)
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go

# ------------------ 3) Setup ------------------
OUTDIR = "/content/esg_results"
os.makedirs(OUTDIR, exist_ok=True)

nlp_spacy = spacy.load("en_core_web_sm", disable=["textcat"])
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words("english"))

# ------------------ 4) Load Models ------------------
print("Loading models... (this may take 2-3 minutes)")
embedder = SentenceTransformer('all-MiniLM-L6-v2')

# FinBERT for sentiment
finbert_tok = AutoTokenizer.from_pretrained("yiyanghkust/finbert-tone")
finbert_mod = AutoModelForSequenceClassification.from_pretrained("yiyanghkust/finbert-tone")
finbert_pipe = pipeline("sentiment-analysis", model=finbert_mod, tokenizer=finbert_tok, return_all_scores=True, truncation=True)

# BART for summarization
summ_tok = AutoTokenizer.from_pretrained("facebook/bart-large-cnn")
summ_mod = AutoModelForSeq2SeqLM.from_pretrained("facebook/bart-large-cnn")
summ_pipe = pipeline("summarization", model=summ_mod, tokenizer=summ_tok, truncation=True)

# ------------------ 5) Helper Functions ------------------
def extract_text_from_pdf(path):
    doc = fitz.open(path)
    return " ".join([page.get_text("text") for page in doc])

def preprocess_text(txt):
    txt = txt.lower()
    txt = re.sub(r'[^a-z0-9\s\.\,\-]', ' ', txt)
    tokens = nltk.word_tokenize(txt)
    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words and len(t) > 2]
    return " ".join(tokens)

def detect_sections(text):
    paras = [p.strip() for p in re.split(r'\n{1,}', text) if p.strip()]
    secmap = {"environment": [], "social": [], "governance": [], "general": []}
    current = "general"
    for p in paras:
        l = p.lower()
        if any(w in l for w in ["environment","climate","carbon","energy","emission"]): current="environment"
        elif any(w in l for w in ["social","community","employee","health","diversity"]): current="social"
        elif any(w in l for w in ["governance","ethic","board","risk","compliance"]): current="governance"
        secmap[current].append(p)
    return {k: " ".join(v) for k, v in secmap.items() if v}

def finbert_sentiment(text, chunk_size=400):
    words = text.split()
    if not words:
        return {"positive": 0, "neutral": 1, "negative": 0}
    scores = {"positive": [], "neutral": [], "negative": []}
    for i in range(0, len(words), chunk_size):
        piece = " ".join(words[i:i+chunk_size])
        try:
            res = finbert_pipe(piece[:1000])[0]
        except Exception:
            res = [{"label": "neutral", "score": 1.0}]
        for r in res:
            if r["label"].lower() in scores:
                scores[r["label"].lower()].append(r["score"])
    avg = {k: float(np.mean(v)) if v else 0.0 for k, v in scores.items()}
    total = sum(avg.values()) or 1.0
    return {k: v / total for k, v in avg.items()}

def esg_score(sentiments):
    weights = {"environment": 0.4, "social": 0.3, "governance": 0.3}
    score = 0
    for k, v in sentiments.items():
        val = v["positive"] - v["negative"]
        score += val * weights.get(k, 0.1)
    return round(((score + 1) / 2) * 100, 2)

def np_encoder(obj):
    if isinstance(obj, np.generic):
        return obj.item()

def plot_sentiment(sentiments, company_name):
    df = pd.DataFrame(sentiments).T
    df.plot(kind='bar', stacked=True, figsize=(8, 4), color=["green", "grey", "red"])
    plt.title(f"Sentiment Analysis - {company_name}")
    plt.ylabel("Proportion")
    plt.xticks(rotation=0)
    plt.show()

def plot_esg_score(score, company_name):
    fig = go.Figure(go.Indicator(
        mode="gauge+number",
        value=score,
        title={'text': f"ESG Score - {company_name}"},
        gauge={
            'axis': {'range': [0, 100]},
            'bar': {'color': "darkgreen"},
            'steps': [
                {'range': [0, 50], 'color': 'red'},
                {'range': [50, 75], 'color': 'yellow'},
                {'range': [75, 100], 'color': 'green'}
            ]
        }
    ))
    fig.show()

# ------------------ 6) Upload PDFs ------------------
from google.colab import files
print("üìÑ Upload ESG PDF(s):")
uploaded = files.upload()

docs = {}
for fname in uploaded.keys():
    print("Extracting:", fname)
    raw = extract_text_from_pdf(fname)
    docs[os.path.splitext(fname)[0]] = {
        "raw_text": raw,
        "clean_text": preprocess_text(raw)
    }

# ------------------ 7) TF-IDF + LDA ------------------
tfidf = TfidfVectorizer(max_features=1500, ngram_range=(1, 2))
tfidf_mat = tfidf.fit_transform([d["clean_text"] for d in docs.values()])
vocab = np.array(tfidf.get_feature_names_out())

tokenized = [t.split() for t in [d["clean_text"] for d in docs.values()]]
dictionary = corpora.Dictionary(tokenized)
corpus_g = [dictionary.doc2bow(t) for t in tokenized]
lda_model = models.LdaModel(corpus=corpus_g, id2word=dictionary, num_topics=3, passes=6, random_state=42)

# ------------------ 8) Sentiment + Summarization ------------------
for name, data in docs.items():
    secs = detect_sections(data["raw_text"])
    docs[name]["sections"] = secs
    docs[name]["sentiments"] = {s: finbert_sentiment(preprocess_text(txt)) for s, txt in secs.items()}
    text_sum = " ".join(list(secs.values())[:3])[:3500]
    try:
        docs[name]["summary"] = summ_pipe(text_sum, max_length=160, min_length=80, do_sample=False)[0]["summary_text"]
    except Exception:
        docs[name]["summary"] = text_sum[:300]

# ------------------ 9) Embeddings + Similarity ------------------
names = list(docs.keys())
embs = embedder.encode([d["clean_text"] for d in docs.values()], convert_to_numpy=True)
sim_df = pd.DataFrame(cosine_similarity(embs), index=names, columns=names)

# ------------------ 10) ESG Scoring + Top Words ------------------
for i, n in enumerate(names):
    docs[n]["esg_score"] = esg_score(docs[n]["sentiments"])
    vals = tfidf_mat[i].toarray().ravel()
    idx = vals.argsort()[-15:][::-1]
    docs[n]["top_words"] = list(zip(vocab[idx], vals[idx]))

# ------------------ 11) Visualizations ------------------
for n in names:
    plt.figure(figsize=(12, 4))
    wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(docs[n]["top_words"]))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.title(f"Top Terms - {n}")
    plt.show()

    plot_sentiment(docs[n]["sentiments"], n)
    plot_esg_score(docs[n]["esg_score"], n)

# ------------------ 12) Save JSON ------------------
os.makedirs(OUTDIR, exist_ok=True)
out_json = {}
for n in names:
    out_json[n] = {
        "esg_score": docs[n]["esg_score"],
        "summary": docs[n]["summary"],
        "sentiments": docs[n]["sentiments"],
        "top_words": docs[n]["top_words"],
        "lda_topics": lda_model.print_topics(num_words=6)
    }

with open(os.path.join(OUTDIR, "esg_results.json"), "w") as f:
    json.dump(out_json, f, indent=2, default=np_encoder)

for n in names:
    print("="*90)
    print(f"üè¢ {n} | ESG Score: {docs[n]['esg_score']}")
    print("Summary:", docs[n]["summary"])
    print("Sentiments:", docs[n]["sentiments"])
    print("Top Keywords:", [w for w, _ in docs[n]["top_words"][:8]])
    print("="*90)

print("\n‚úÖ Analysis complete. Results saved to /content/esg_results/esg_results.json")

import json
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import plotly.graph_objects as go
import pandas as pd

# ---------------- Load JSON ----------------
with open("/content/esg_results/esg_results.json") as f:
    data = json.load(f)

for company, info in data.items():
    print(f"\nüè¢ {company} | ESG Score: {info['esg_score']}\n")

    # 1Ô∏è‚É£ Sentiment Stacked Bar
    sentiments = info["sentiments"]
    df_sent = pd.DataFrame(sentiments).T
    df_sent.plot(kind='bar', stacked=True, figsize=(8,4), color=["green","grey","red"])
    plt.title(f"Sentiment Analysis - {company}")
    plt.ylabel("Proportion")
    plt.xticks(rotation=0)
    plt.show()

    # 2Ô∏è‚É£ ESG Score Gauge
    fig = go.Figure(go.Indicator(
        mode = "gauge+number",
        value = info["esg_score"],
        title = {'text': f"ESG Score - {company}"},
        gauge = {'axis': {'range': [0,100]},
                 'bar': {'color': "darkgreen"},
                 'steps' : [
                     {'range':[0,50], 'color':'red'},
                     {'range':[50,75], 'color':'yellow'},
                     {'range':[75,100], 'color':'green'}]}))
    fig.show()

    # 3Ô∏è‚É£ Top Words Horizontal Bar
    top_words = info["top_words"][:15]  # Top 15
    words, scores = zip(*top_words)
    plt.figure(figsize=(10,6))
    sns.barplot(x=list(scores), y=list(words), palette="viridis")
    plt.title(f"Top Words in {company}")
    plt.xlabel("TF-IDF Score")
    plt.ylabel("Word")
    plt.show()

    # 4Ô∏è‚É£ Word Cloud
    wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(top_words))
    plt.figure(figsize=(12,5))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(f"Word Cloud - {company}")
    plt.show()

    # 5Ô∏è‚É£ LDA Topics
    topics = info["lda_topics"]
    for t_id, t_content in topics:
        terms = [x.split('*')[1].replace('"','') for x in t_content.split(' + ')]
        weights = [float(x.split('*')[0]) for x in t_content.split(' + ')]
        plt.figure(figsize=(8,4))
        sns.barplot(x=weights, y=terms, palette='coolwarm')
        plt.title(f"Topic {t_id} - {company}")
        plt.xlabel("Weight")
        plt.show()

